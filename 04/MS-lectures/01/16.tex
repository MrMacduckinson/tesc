\subsection{%
  Лекция \texttt{24.??.??}.%
}

\subheader{Энтропия}

Пусть случайная величина \(\xi\) это результат некоторого эксперимента с \(N\)
исходами \(A_1, \dotsc, A_N\). Вероятности этих исходов~--- \(p_1, \dotsc,
p_N\).

\begin{definition}
  Энтропией эксперимента называется величина

  \begin{equation*}
    H (\xi) = -\sum_{i = 1}^N p_i \log_2 p_i 
  \end{equation*}

  При \(p_i = 0\) соответствующее слагаемое полагаем равным нулю.
\end{definition}

\begin{remark}
  \(H (\xi) \ge 0\), т.к. \(\log_2 p_i < 0\), то каждое слагаемое будет
  неположительным. Минус перед знаком суммы дает общую неотрицательность
  энтропии.
\end{remark}

\begin{remark}
  \(H (\xi) = 0\) тогда и только тогда, когда \(\exists i\) такое, что \(p_i =
  1\) и \(p_k = 0\) при \(k \neq i\). В этом случае результат эксперимента не
  случаен, а полностью предопределен (нет неопределенности результата).
\end{remark}

\begin{lemma}
  Максимум энтропии, равный \(\log_2 N\), достигается при \(p_1 = \dotsc = p_N =
  \frac{1}{n}\).
\end{lemma}

\begin{proof}
  Рассмотрим дискретную случайную величину \(\eta\), значениями которой будут
  \(p_1, \dotsc, p_N\), а вероятности этих значений все будут равны
  \(\frac{1}{N}\). Рассмотрим функцию \(\phi(x) = x \log_2 x\) и найдем ее
  вторую производную.

  \begin{equation*}
    \phi''(x)
    = \prh{\log_2 x + x \cdot \frac{1}{x \ln 2}}'
    = \frac{1}{x \ln 2}
  \end{equation*}

  Это больше нуля при \(x > 0\), значит эта функция выпукла вниз. Тогда по
  неравенству Йенсена получаем

  \begin{equation*}
    \begin{aligned}
      \phi \prh{\expected{\eta}} \le \expected{\phi(\eta)}
    \\
      \phi \prh{\expected{\eta}}
      = \phi \prh{\sum_{i = 1}^N p_i \frac{1}{N}}
      = \phi \prh{\frac{1}{N}}
      = -\frac{1}{N} \log_2 N
    \\
      \expected{\phi(\eta)}
      = \sum_{i = 1}^N \frac{1}{N} p_i \log_2 p_i
      = \frac{1}{N} \sum_{i = 1}^N p_i \log_2 p_i
      = \frac{1}{N} H (\xi)
    \\
      -\frac{1}{N} \log_2 N \le \frac{1}{N} H(\xi)
    \\
      \log_2 N \ge H(\xi)
    \end{aligned}
  \end{equation*}
\end{proof}

\begin{remark}
  Энтропию можно рассматривать как количественную характеристику
  неопределенности эксперимента. Если \(H = 0\), то результат предопределен, а
  если \(H = H_{\max} = \log_2 N\), то все исходы равновероятны и ни одному из
  них нельзя отдать предпочтение.
\end{remark}

\begin{example}
  Пусть \(\xi \in \Bdist{p}\), тогда

  \begin{equation*}
    H (\xi) = -(1 - p) \log_2 (1 - p) - p \log_2 p
  \end{equation*}

  При \(p = 0.5\) имеем

  \begin{equation*}
    H (\xi)
    = -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2}
    = 1
  \end{equation*}
\end{example}

\begin{example}
  Перед испытуемым зажигалось \(n\) лампочек, и он должен был быстро указать на
  загоревшуюся лампочку. Оказалось, что самое длинное время реакции было в том
  случае, когда лампочки загорались с одинаковой частотой. Если частоты были
  разными, то время реакции было прямопропорционально энтропии эксперимента.
\end{example}

\subheader{Кодирование информации}

Пусть требуется передать сообщение из \(N\) символов. Пусть длины кодовых слов
равны, тогда для каждого символа понадобится \(\log_2 N\) бит, а для всего
сообщения потребуется \(N \log_2 N\) бит. Для больших по объему сообщений можно
сократить число бит, учитывая что разные символы возникают с разной частотой.
Пусть \(p_1, \dotsc, p_N\)~--- вероятности соответствующих \(N\) символов, тогда
при длинном сообщении \(i\)-ый символ встречается \(v_i = N p_i\) раз.

\begin{definition}
  Назовем сообщение типичным, если \(\forall i \colon \abs{v_i - N p_i} <
  \delta\).
\end{definition}

Обозначим \(M_{N, \delta}\)~--- число типичных сообщений.

\begin{theorem}[Макмиллана (частный случай)]
  \begin{equation*}
    \frac{1}{N} \log_2 M_{N, \delta} \Rarr{n \to \infty} H
  \end{equation*}
\end{theorem}

Отсюда число типичных сообщений \(M_{N, \delta} \le 2^{N (H + \epsilon)}\), где
\(\epsilon > 0\)~--- мало. Обозначим \(H_0 = \log_2 N\), тогда с вероятностью,
близкой к единице, можно сократить длину сообщения с коэффициентом сжатия
\(\gamma = \frac{H}{H_0}\). Если символы появляются независимо, то большее
сжатие невозможно, а если использовать их зависимость, то коэффициент сжатия
можно существенно уменьшить. Пусть \(\gamma_{\infty}\)~--- данный коэффициент
сжатия. Для русского языка \(\gamma \approx 0.87\), а \(\gamma_{\infty} \approx
0.24\) (для литературного языка) и \(\gamma_{\infty} \approx 0.17\) (для деловых
сообщений).

\begin{definition}
  Величина \(1 - \gamma_{\infty}\) называется избыточностью языка.
\end{definition}

\subheader{Энтропия абсолютно непрерывных распределений}

\begin{definition}
  Пусть \(\xi\) это абсолютно непрерывная случайная величина с плотностью
  \(f(x)\). Энтропией случайной величины \(\xi\) называется величина

  \begin{equation*}
    H(\xi) = -\int_{-\infty}^{\infty} f(x) \log_2 f(x) \dd x
  \end{equation*}
\end{definition}

\begin{theorem}
  Следующие распределения имеют наибольшую энтропию:
  
  \begin{enumerate}
  \item
    Если \(\xi \in \segment{0}{1}\), то \(\xi \in \udist{0}{1}\).
  
  \item
    Если \(\xi \in [0; +\infty)\) и \(\expected{\xi} = 1\), то \(\xi \in
    \Edist{1}\).

  \item
    Если \(\xi \in (-\infty; +\infty)\), \(\expected{\xi} = 0\) и
    \(\variance{\xi} = 1\), то \(\xi \in \ndist{0}{1}\).
  \end{enumerate}
\end{theorem}

\subheader{Задача о разорении игрока}

Играют два игрока. Вероятность выигрыша первого игрока~--- \(p\), а второго~---
\(1 - p\). Ставка в каждой игре это одна единица. Капитал первого игрока равен
\(k\), а второго~--- \(M - k\). Игра прекращается, когда один из игроков
потеряет свой капитал. Найти вероятность разорения первого игрока.

Выигрыш первого игрока \(S_n = z_1 + \dotsc + z_n\), где \(\prob{z_i = 1} = p\)
и \(\prob{z_i = -1} = q\). Пусть \(r_k\) это вероятность разорения при стартовом
капитале \(k\), тогда \(r_k = p r_{k + 1} + q r_{k - 1}\). Это линейное
однородное рекуррентное уравнение. Сначала рассмотрим случай \(p \neq
\frac{1}{2}\). Составим и решим характеристическое уравнение.

\begin{equation*}
  p x^2 - x + (1 - p) = 0
  \implies
  x_{1, 2}
    = \frac{1 \pm \sqrt{1 - 4 p (1 - p)}}{2 p}
    = \frac{1 \pm (1 - 2 p)}{2 p}
    = \begin{cases}
      \frac{1 - p}{p} = \lambda \\ 1
    \end{cases}
  \implies
  r_k = c_1 1^k + c_2 \lambda^k
\end{equation*}

Это общее решение. Чтобы найти константы подставим начальные (в данном случае их
лучше назвать граничными) условия.

\begin{equation*}
  \begin{cases}
    r_0 = 1 = c_1 + c_2 \\
    r_M = 0 = c_1 + c_2 \lambda^M
  \end{cases}
  \iff
  \begin{cases}
    c_1 = 1 - c_2 \\
    1 - c_2 + c_2 \lambda^M = 0
  \end{cases}
  \iff
  \begin{cases}
    c_2 = \frac{1}{1 - \lambda^M} \\
    c_1 = \frac{-\lambda^M}{1 - \lambda^M}
  \end{cases}
\end{equation*}

Таким образом получаем итоговое решение

\begin{equation*}
  r_k
  = \frac{-\lambda^M}{1 - \lambda^M} + \frac{\lambda^k}{1 - \lambda^M}
  = \frac{\lambda^k - \lambda^M}{1 - \lambda^M}
\end{equation*}

Далее рассмотрим ситуацию \(M \to \infty\). Возможны два случая.

\begin{enumerate}
\item
  \(p < \frac{1}{2}\), тогда \(\lambda > 1\) и \(\lambda^M \to \infty\), значит

  \begin{equation*}
    r_k
    = \frac{\lambda^k - \lambda^M}{1 - \lambda^M}
    = \frac{\lambda^{k - M} - 1}{\lambda^{-M} - 1}
    = 1
  \end{equation*}

  Таким образом игрок гарантированно разоряется.

\item
  \(p > \frac{1}{2}\), тогда \(\lambda < 1\) и \(\lambda^M \to 0\), значит

  \begin{equation*}
    r_k
    = \frac{\lambda^k - \lambda^M}{1 - \lambda^M}
    = \lambda^k
    = \prh{\frac{q}{p}}^k
  \end{equation*}
\end{enumerate}

Вернемся к решению характеристического уравнения и рассмотрим случай \(p =
\frac{1}{2}\). Тогда \(x_1 = x_2 = 1\) и общее решение будет иметь вид

\begin{equation*}
  r_k
  = c_1 1^k + c_2 k \cdot 1^k
  = c_1 + c_2 k
\end{equation*}

Для нахождения констант подставим начальные условия.

\begin{equation*}
  \begin{cases}
    r_0 = 1 = c_1 \\
    r_M = 0 = c_1 + c_2 M
  \end{cases}
  \iff
  \begin{cases}
    c_1 = 1 \\
    c_2 = -\frac{1}{M}
  \end{cases}
  \implies
  r_k = 1 - \frac{k}{M}
\end{equation*}

При \(M \to \infty\) получаем, что \(r_k \to 1\). Значит вне зависимости от
стартового капитала при бесконечной игре первый игрок рано или поздно разорится.
