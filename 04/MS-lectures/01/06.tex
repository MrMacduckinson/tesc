\subsection{%
  Лекция \texttt{24.??.??}.%
}

\subheader{Проверка статистических гипотез}

\begin{definition}
  Гипотезой \(H\) называется предположение о распределении случайной величины.
\end{definition}

\begin{definition}
  Гипотеза называется простой, если она однозначно определяет распределение,
  т.е. \(H \colon \mathcal{F} \in \mathcal{F}_1\), где \(\mathcal{F}_1\)~---
  распределение известного типа с известными параметрами. Все остальные гипотезы
  называются сложными. Они состоят из конечного или бесконечного числа простых
  гипотез.
\end{definition}

Будем изучать лишь самую простую схему из двух гипотез, где \(H_0\)~--- это
основная (нулевая) гипотеза, а \(H_1\) это альтернативная (конкурирующая)
гипотеза, состоящая в том, что основная гипотеза неверна. Стоит отметить, что
иногда рассматриваются более сложные схемы, состоящие из набора нескольких
гипотез.

Основная гипотеза \(H_0\) принимается или отклоняется при помощи статистики
критерия \(K\), где \(K \prh{X_1, \dotsc, X_n} \to \RR = \bar{S} \cup S\).
Принимаем гипотезу \(H_0\), если статистика критерия попала в область
\(\bar{S}\), в противном случае принимаем гипотезу \(H_1\).

\begin{definition}
  Область \(S\) называется критической областью критерия.
\end{definition}

\begin{definition}
  Точки \(\tcrit\) на границе двух областей называются критическими.
\end{definition}

\begin{definition}
  Ошибка первого рода состоит в том, что нулевая гипотеза \(H_0\) отклоняется,
  хотя она верна.
\end{definition}

\begin{definition}
  Ошибка второго рода состоит в том, что отклоняется альтернативная гипотеза
  \(H_1\), хотя она верна.
\end{definition}

\begin{definition}
  Вероятность \(\alpha\) ошибки первого рода называется уровнем значимости
  критерия.
\end{definition}

\begin{definition}
  Вероятность ошибки второго рода обозначается \(\beta\). Мощностью критерия
  называется вероятность \(1 - \beta\), т.е. вероятность не допущения ошибки
  второго рода.
\end{definition}

\begin{remark}
  Заметим, что вероятности \(\alpha\) и \(\beta\) не связаны каким-либо
  соотношением.
\end{remark}

\begin{remark}
  Ясно, что критерий будет тем лучше, чем меньше вероятности ошибок \(\alpha\) и
  \(\beta\). При увеличении объема выборки обе эти вероятности уменьшаются,
  однако при фиксированном объеме выборки попытки уменьшить одну вероятность
  ведут к увеличению другой.
\end{remark}

\subheader{Способы сравнения критериев}

Пусть имеется два критерия \(K_1\) и \(K_2\) с соответствующими вероятностями
ошибок \(\alpha_1, \beta_1\) и \(\alpha_2, \beta_2\). Объем выборки \(n\)
фиксированный.

\subsubheader{I.}{Минимаксный подход}

Критерий \(K_1\) не хуже, чем \(K_2\), если \(\max (\alpha_1, \beta_1) < \max
(\alpha_2, \beta_2)\). Из всех критериев выбираем тот, который не хуже
остальных.

\subsubheader{II.}{Байесовский подход}

Пусть известны потери \(h_1\) и \(h_2\) от ошибок первого и второго рода. Тогда
средние ожидаемые потери составят \(u = \alpha h_1 + \beta h_2\). Из всех
критериев выбираем тот, у которого средние ожидаемые потери наименьшие.

\subsubheader{III.}{Выбор наиболее мощного критерия}

Обозначим \(K_{\epsilon} = \set{ K_i \given \alpha \le \epsilon }\), т.е. класс
критериев данной выборки.

\begin{definition}
  Критерий \(K \in K_{\epsilon}\) называется наиболее мощным критерием уровня
  \(\epsilon\), если \(\beta \le \beta_i \given \forall K_i \in K_{\epsilon}\).
\end{definition}

\subheader{Построение критериев согласия}

\begin{definition}
  Говорят, что критерий \(K\) является критерием асимптотического уровня
  \(\epsilon\), если \(\alpha \to \epsilon\) при \(n \to \infty\).
\end{definition}

\begin{definition}
  Критерий \(K\) для проверки гипотезы \(H_0\) против альтернативной гипотезы
  \(H_1\) называется состоятельным, если \(\beta \to 0\) при \(n \to \infty\).
\end{definition}

\begin{definition}
  Критерием согласия уровня \(\epsilon\) называется состоятельный критерий
  асимптотического уровня \(\epsilon\).
\end{definition}

В качестве критерия согласия берется статистика \(K \prh{X_1, \dotsc, X_n}\) со
свойствами

\begin{enumerate}
\item
  Если \(H_0\) верна, то \(K \prh{X_1, \dotsc, X_n} \rightrightarrows z\), где
  \(z\)~--- случайная величина с известным распределением.

\item
  Если \(H_0\) неверна, то \(K \prh{X_1, \dotsc, X_n} \Rarr{\probP} \infty\) при
  \(n \to \infty\).
\end{enumerate}

Для заданного уровня значимости \(\epsilon\) находим квантиль \(\tcrit\) такой,
что \(\prob{\abs{z} \ge \tcrit} = \epsilon\). В результате получаем критерий с
уровнем значимости \(\alpha\). Итого

\begin{equation*}
  \begin{cases}
    H_0, & \text{ если } \abs{K} < \tcrit \\
    H_1, & \text{ если } \abs{K} \ge \tcrit
  \end{cases}
\end{equation*}

\begin{remark}
  Таким образом в качестве статистики берется функция отклонения эмпирического
  распределения от теоретического. Если нулевая гипотеза верна, то эта функция
  сходится к собственному распределению, если нет, то неограниченно возрастает.
\end{remark}

\begin{lemma}
  Построенный критерий обладает свойствами

  \begin{enumerate}
  \item
    Это критерий асимптотического уровня \(\epsilon\).

  \item
    Этот критерий состоятельный.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Пусть гипотеза \(H_0\) верна. Тогда по первому свойству выбранной статистики
  имеем \(\forall x \given F_K (x) \to F_z (x)\) при \(n \to \infty\). Значит

  \begin{equation*}
    \alpha
    = \prob{\abs{K} \ge \tcrit \given H_0}
    = 1 - \prh{F_K \prh{\tcrit} - F_K \prh{-\tcrit}}
    \Rarr{n \to \infty}
    1 - \prh{F_z \prh{\tcrit} - F_z \prh{-\tcrit}}
    = \prob{\abs{z} \ge \tcrit}
    = \epsilon
  \end{equation*}

  Далее покажем состоятельность критерия. По второму свойству выбранной
  статистики получаем, что если \(H_1\) верна, то \(\abs{K} \Rarr{\probP}
  \infty\), т.е. \(\forall c \in \RR \given \prob{\abs{K} \ge c} \to 1\), значит
  \(\prob{\abs{K} < c} \to 0\).
\end{proof}

\subheader{Гипотеза о среднем нормальной совокупности с известной дисперсией}

Пусть имеется выборка \(\sample{X} = \prh{X_1, \dotsc, X_n}\) объема \(n\)
случайной величины \(X \in \ndist{a}{\sigma^2}\), где \(\sigma^2\) это известное
значение. Проверяется гипотеза \(H_0\), которая состоит в том, что \(a = a_0\).
Альтернативная гипотеза \(H_1\) состоит в том, что \(a \neq a_0\).

В качестве статистики критерия возьмем функцию \(\display{K = \sqrt{n} \cdot
\frac{\avg{x} - a_0}{\sigma}}\). Проверим, что она имеет требуемые свойства.

\begin{enumerate}
\item
  Если \(H_0\) верна, то

  \begin{equation*}
    K
    = \sqrt{n} \cdot \frac{\avg{x} - a_0}{\sigma}
    = \sqrt{n} \cdot \frac{\avg{x} - a}{\sigma}
    \in \ndist{0}{1}
  \end{equation*}

  по \ref{lem:base-theorem-1}.

\item
  Если \(H_0\) неверна, т.е. \(a \neq a_0\), то

  \begin{equation*}
    \abs{K}
    = \abs{\sqrt{n} \cdot \frac{\avg{x} - a_0}{\sigma}}
    = \abs{
      \sqrt{n} \cdot \frac{\avg{x} - a}{\sigma}
      + \sqrt{n} \cdot \frac{a - a_0}{\sigma}
    }
  \end{equation*}

  Первое слагаемое по \ref{lem:base-theorem-1} имеет стандартное нормальное
  распределение, поэтому оно будет ограничено по вероятности. Второе слагаемое
  будет стремиться к бесконечности, т.к. \(\sqrt{n} \to \infty\) при \(n \to
  \infty\), а остальная часть этого слагаемого это какая ненулевая константа в
  силу того, что \(a \neq a_0\). Итого \(\abs{K} \Rarr{\probP} \infty\) при \(n
  \to \infty\).
\end{enumerate}

Итак, получили следующий критерий: для уровня значимости \(\alpha\) выберем
\(\tcrit\) такую, что

\begin{equation*}
  \alpha
  = \prob{\abs{K} \ge \tcrit}
  = 1 - 2 \Phi \prh{\tcrit}
  \implies
  \Phi \prh{\tcrit}
  = \frac{1 - \alpha}{2}
\end{equation*}

Значит \(\tcrit\) это обратное значение функции Лапласа в точке \(\frac{1 -
\alpha}{2}\) или квантиль уровня \(1 - \frac{\alpha}{2}\) стандартного
нормального распределения. Получаем критерий согласия

\begin{equation*}
  \begin{cases}
    H_0, & \abs{K} < \tcrit \\
    H_1, & \abs{K} \ge \tcrit
  \end{cases}
\end{equation*}

\subheader{Гипотеза о среднем нормальной совокупности при неизвестной дисперсии}

Пусть имеется выборка \(\sample{X} = \prh{X_1, \dotsc, X_n}\) объема \(n\)
случайной величины \(X \in \ndist{a}{\sigma^2}\). Проверяется гипотеза \(H_0\),
которая состоит в том, что \(a = a_0\). Альтернативная гипотеза \(H_1\) состоит
в том, что \(a \neq a_0\).

В качестве статистики критерия возьмем функцию \(\display{K = \sqrt{n} \cdot
\frac{\avg{x} - a_0}{S}}\). Проверим, что она имеет требуемые свойства.

\begin{enumerate}
\item
  Если \(H_0\) верна, то

  \begin{equation*}
    K
    = \sqrt{n} \cdot \frac{\avg{x} - a_0}{S}
    = \sqrt{n} \cdot \frac{\avg{x} - a}{S}
    \in \Tdist{n - 1}
  \end{equation*}

  по \ref{lem:base-theorem-4}.

\item
  Если \(H_0\) неверна, т.е. \(a \neq a_0\), то

  \begin{equation*}
    \abs{K}
    = \abs{\sqrt{n} \cdot \frac{\avg{x} - a_0}{S}}
    = \abs{
      \sqrt{n} \cdot \frac{\avg{x} - a}{S}
      + \sqrt{n} \cdot \frac{a - a_0}{S}
    }
  \end{equation*}

  Аналогично предыдущей гипотезе получаем, что \(\abs{K} \Rarr{\probP} \infty\)
  при \(n \to \infty\).
\end{enumerate}

Итак, получили следующий критерий: для уровня значимости \(\alpha\) выберем
\(\tcrit\) такую, что \(\display{\prob{\abs{t_{n - 1}} \ge \tcrit} = \alpha}\).
Таким образом \(\tcrit\) это квантиль уровня \(\frac{1 - \alpha}{2}\)
распределения \(\Tdist{n - 1}\) или \(\tcrit\) это квантиль уровня значимости
\(\alpha\) двустороннего распределения \(\abs{\Tdist{n - 1}}\). Получаем
критерий согласия

\begin{equation*}
  \begin{cases}
    H_0, & \abs{K} < \tcrit \\
    H_1, & \abs{K} \ge \tcrit
  \end{cases}
\end{equation*}

\subheader{Доверительные интервалы как критерии гипотез по параметрам
распределения}

Пусть имеется выборка \(\sample{X} = \prh{X_1, \dotsc, X_n}\) объема \(n\) из
\(X \in \mathcal{F}_{\Theta}\), где \(\mathcal{F}_{\Theta}\) это распределение
известного типа, с неизвестным параметром \(\Theta\). Проверяется гипотеза
\(H_0\) о том, что \(\Theta = \Theta_0\), против альтернативной гипотезы
\(\Theta \neq \Theta_0\). Допустим, что для параметра \(\Theta\) построен
доверительный интервал \(\interval{\Theta_{\gamma}^-}{\Theta_{\gamma}^+}\)
надежности \(\gamma\).

\begin{lemma}
  В описанных условиях получаем критерий 

  \begin{equation*}
    \begin{cases}
      H_0, & \Theta_0 \in \interval{\Theta_{\gamma}^-}{\Theta_{\gamma}^+} \\
      H_1, & \Theta_0 \notin \interval{\Theta_{\gamma}^-}{\Theta_{\gamma}^+}
    \end{cases}
  \end{equation*}

  уровня \(\alpha = 1 - \gamma\).
\end{lemma}

\begin{proof}
  \begin{equation*}
    \alpha
    = \prob{
      \Theta_0 \notin \interval{\Theta_{\gamma}^-}{\Theta_{\gamma}^+}
      \given X \in \mathcal{F}_{\Theta_0}
    }
    = 1 - \prob{
      \Theta_0 \in \interval{\Theta_{\gamma}^-}{\Theta_{\gamma}^+}
      \given X \in \mathcal{F}_{\Theta_0}
    }
    = 1 - \gamma
  \end{equation*}
\end{proof}

\begin{remark}
  Доказать, что данный критерий будет состоятельным, в общем случае нельзя.
\end{remark}

\begin{example}
  По выборке объема \(n = 36\) из нормальной совокупности с известным средним
  квадратическим отклонением \(\sigma = 1.44\) найдено выборочное среднее
  \(\avg{x} = 21.6\). Проверить гипотезу \(H_0\) о том, что \(a = 21\), против
  альтернативной гипотезы \(H_1\) о том, что \(a \neq 21\). Уровень значимости
  принять \(\alpha = 0.05\).

  \solution{} Составляем статистику

  \begin{equation*}
    K
    = \sqrt{n} \cdot \frac{\avg{x} - a_0}{\sigma}
    = \sqrt{36} \cdot \frac{21.6 - 21}{1.44}
    = 2.5
  \end{equation*}

  Теперь ищем критическую точку.

  \begin{equation*}
    \Phi \prh{\tcrit}
    = \frac{1 - \alpha}{2}
    = 0.475
    \implies \tcrit = 1.96
  \end{equation*}

  Т.к. \(\abs{K} > \tcrit\), то основная гипотеза \(H_0\) отклоняется, и
  принимается альтернативная гипотеза \(H_1\).
\end{example}

\subheader{Некоторые дополнения с практики}

\begin{theorem}
  Если \(f_{\xi} (\Me) \neq 0\), то \(\sMe \Rarr{\probP} \Me\), причем со
  скоростью \(\frac{1}{\sqrt{n}}\). Таким образом выборочная медиана является
  состоятельной асимптотически нормальной оценкой теоретической медианы.
\end{theorem}

В случае симметричных распределений выборочную медиану удобно использовать, если
распределение имеет \quote{жирные хвосты}. В случае \quote{жирных} хвостов
возможны так называемые \quote{выбросы}, которые заметно повлияют на выборочное
среднее, и поэтому его не желательно использовать в качестве оценки центра
симметрии. В случае \quote{жирных хвостов} или \quote{выбросов} можно
использовать медиану или иные методики.

Недостатки медианы:

\begin{enumerate}
\item
  Хорошо работает только в случае симметричных распределений.

\item
  В случае нормального распределения медиана сходится приблизительно на \(20
  \%\) медленнее, чем среднее выборочное, и до \(40 \%\) медленнее в некоторых
  других случаях.
\end{enumerate}

Подумаем над устранением этих недостатков. Пусть распределение симметрично.
Рассмотрим два метода.

\subsubheader{I.}{Метод усеченного среднего}

Метод заключается в том, что мы отбрасываем из выборки сверху и снизу по
\(5\)---\(10\) значений или по \(5 \%\) значений.

Пусть дана выборка \(\sample{X} = \prh{X_1, \dotsc X_n}\) объема \(n\).
Допустим, что мы отбросили по \(k\) значений сверху и снизу и получили выборку
объема \(n - 2 k\) вида \(\prh{X_{(k + 1)}, \dotsc, X_{(n - k)}} \). Вычислим
среднее выборочное этой выборки \(\display{\frac{\sum_{i = k + 1}^{n - k} X_i}{n
- 2 k}}\). Это компромисс между средним выборочным и медианой, т.к.

\begin{enumerate}
\item
  При \(k = 0\) получаем \(\avg{x}\).

\item
  При \(n - 2 k = 1\) получаем \(\Me\).
\end{enumerate}

\subsubheader{II.}{Метод средних Уолша}

Пусть дана выборка \(\sample{X} = \prh{X_1, \dotsc X_n}\) объема \(n\). Для всех
пар считаем среднее арифметическое \(y_k = \frac{1}{2} (x_i + x_j)\), где \(1
\le k \le \frac{1}{2} n (n + 1)\). Далее находим медиану полученной новой
выборки. Преимущества данного метода:

\begin{enumerate}
\item
  Эффективность по сравнению со среднем выборочным упадет не более, чем на \(12
  \%\).

\item
  Сглаживает выбросы.
\end{enumerate}
