\subsection{%
  Лекция \texttt{24.??.??}.%
}

\subheader{Статистическая зависимость}

\begin{definition}
  Зависимость называется статистической, если изменение одной случайной величины
  вызывает изменение распределения другой. Если при этом изменяется среднее
  значение другой случайной величины, то такая статистическая зависимость
  называется корреляционной. Если при увеличении одной случайной величины,
  среднее другой также увеличивается, то имеет место прямая корреляция, в
  противном случае~--- обратная корреляция.
\end{definition}

\subheader{Корреляционное облако}

Пусть в ходе \(n\) экспериментов появились значения случайных величин \(X\) и
\(Y\). Нанеся эти точки на координатную плоскость (\figref{01_08_01}) получим
корреляционное облако. По его виду можно сделать некоторые предположения о
наличии и типе корреляции.

\gallerytwo{01_08_01}{Корреляционное облако}
  {Нет зависимости}{Прямая корреляция}

\subheader{Корреляционная таблица}

Пусть даны экспериментальные данные \((x_1, y_1), \dotsc, (x_n, y_n)\). Их
удобно представить в виде корреляционной таблицы, где по горизонтали будут
расположены \(x_i\), по вертикали~--- \(y_i\), а в клетках будут находиться
числа \(n_{i, j}\), показывающие сколько раз встретилась точка \((x_i, y_j)\).

\spreadsheet{01_08_02}{0.65 \linewidth}{X||X|X|X|X|X|X}
  {Корреляционная таблица}{ex-corr-table}

\begin{example}
  Пусть \(n = 50\). Данные представлены в виде корреляционной таблицы
  \ref{tbl:ex-corr-table}. Столбец \(\avg{y}_x\) не входит в корреляционную
  таблицу, но обычно записывается рядом. Это условное среднее, т.е. выборочное
  среднее одной случайной величины при условии, что другая величина
  зафиксирована. Его можно вычислить по формуле

  \begin{equation*}
    \avg{y}_x = \frac{1}{n_x} \sum_i n_{x, y} y_i
    \qquad
    \avg{x}_y = \frac{1}{n_y} \sum_i n_{x, y} n_i
  \end{equation*}

  Т.к. с ростом \(x\) растут условные средние \(\avg{y}_x\), то имеет место
  прямая корреляция.
\end{example}

\begin{remark}
  Если основные частоты сгруппированы относительно главной диагонали, то имеет
  место прямая корреляция, а если относительно побочной, то обратная. Если
  частоты распределены примерно равномерно, то корреляции практически нет.
\end{remark}

\begin{remark}
  При большом объеме данных непрерывных случайных величин \(X\) и \(Y\) данные
  удобно собрать в интервальную корреляционную таблицу, где по вертикали
  отмечаем интервалы \([a_i; a_{i + 1})\) случайной величины \(X\), а по
  вертикали~--- интервалы \([b_j; b_{j + 1})\) случайной величины \(Y\). В
  клетках указываем количество точек \(v_{i, j}\), попавших в соответствующую
  прямоугольную область \([a_i; a_{i + 1}) \times [b_j; b_{j + 1})\).
\end{remark}

\subheader{Критерий \(\chi^2\) для проверки независимости}

Пусть выборка \((x_1, y_1), \dotsc, (x_n, y_n)\) представлена в виде
интервальной корреляционной таблицы. Случайная величина \(X\) при этом разбита
на \(k\) интервалов, а случайная величина \(Y\) на \(m\) интервалов. Обозначим
\(v_{i \cdot}\) число значений \(X\) в интервале \([a_i; a_{i + 1})\), где \(1
\le i \le k\), \(v_{\cdot j}\) число значений \(Y\) в интервале \([b_j; b_{j +
1})\), где \(1 \le j \le m\), а \(v_{i, j}\) это число точек в соответствующем
прямоугольнике \([a_i; a_{i + 1}) \times [b_j; b_{j + 1})\). Данные собрали в
корреляционную таблицу \ref{tbl:corr-table}.

\spreadsheet{01_08_03}{0.9 \linewidth}{X||X|X|X|X|X}
  {Корреляционная таблица}{corr-table}

Проверяется основная гипотеза \(H_0\) о том, что \(X\) и \(Y\)~--- независимы
против альтернативной \(H_1\), а том, что они зависимы. Если гипотеза \(H_0\)
верна, то теоретическая вероятность попадания случайной величины \(\tuple{x,
y}\) в любой прямоугольник равна произведению теоретических вероятностей
попадания этих случайных величин в соответствующие интервалы. Таким образом

\begin{equation*}
  p_{i, j}
  = \prob{X \in [a_{i - 1}; a_i) \text{ и } Y \in [b_{j - 1}; b_j)]}
  = \prob{X \in [a_{i - 1}; a_i)} \prob{Y \in [b_{j - 1}; b_j)]}
  = p_i q_j 
\end{equation*}

Тогда по Закону Больших Чисел

\begin{equation*}
  \frac{v_{i \cdot}}{n} \Rarr{\probP} p_i
  \qquad
  \frac{v_{\cdot j}}{n} \Rarr{\probP} q_j
  \qquad
  \frac{v_{i, j}}{n} \Rarr{\probP} p_{i, j}
\end{equation*}

поэтому основанием для отклонения нулевой гипотезы должна служить заметная
разница между величинами \(\frac{v_{i, j}}{n}\) и \(\frac{v_{i \cdot}}{n} \cdot
\frac{v_{\cdot j}}{n}\). В качестве статистики критерия берется функция

\begin{equation*}
  K
  = n \sum_{i, j} \frac{\prh{v_{i, j} - \frac{1}{n} v_{i \cdot} v_{\cdot j}}^2}
    {v_{i \cdot} v_{\cdot j}}
\end{equation*}

\begin{theorem}
  Если основная гипотеза \(H_0\) верна, то \(K \rightrightarrows \chi_{(k - 1)
  (m - 1)}^2\).
\end{theorem}

Получили критерий: пусть \(\tcrit\) это квантиль распределения \(\Hdist{(n -
1)(m - 1)}\) уровня значимости \(\alpha\), тогда

\begin{equation*}
  \begin{cases}
    H_0, & K < \tcrit \\
    H_1, & K \ge \tcrit
  \end{cases}
\end{equation*}

\begin{remark}
  Частота каждой клетки должна быть не менее пяти.  
\end{remark}

\subheader{Однофакторный дисперсионный анализ}

Предположим, что на случайную величину \(X\) (результат) может влиять фактор
\(Z\), причем \(Z\) не обязательно случайная величина. Требуется определить,
оказывает ли фактор \(Z\) на среднее значение \(X\). Пусть при различных \(k\)
уровнях фактора \(Z\) получены \(k\) независимых выборок случайной величины
\(X\). Обозначим их \(X^{(1)} = \prh{X_1^{(1)}, \dotsc, X_{n_1}^{(1)}}, \dotsc,
X^{(k)} = \prh{X_1^{(k)}, \dotsc, X_{n_k}^{(k)}}\). В общем случае их
распределение отличается, поэтому с формальной точки зрения это выборки
различных случайных величин.

\subheader{Общая межгрупповая и внутригрупповая дисперсия}

Для каждой выборки вычислим ее выборочное среднее и дисперсию.

\begin{equation*}
  \avg{x}^{(j)} = \frac{1}{n_j} \sum_{i = 1}^{n_j} x_i^{(j)}
  \qquad
  \varianceD^{(j)} = \frac{1}{n_j} \sum_{i = 1}^{n_j}
    \prh{x_i^{(j)} - \avg{x}^{(j)}}^2
\end{equation*}

Объединив все выборки в одну общую получаем выборку объема \(n = n_1 + \dotsc +
n_k\). Для нее вычислим общее выборочное среднее и общую выборочную дисперсию

\begin{equation*}
  \avg{x}
  = \frac{1}{n} \sum_{i, j} x_i^{(j)}
  = \frac{1}{n} \sum_{j = 1}^k \avg{x}^{(j)} n_j
  \qquad
  \varianceO = \frac{1}{n} \sum_{i, j} \prh{x_i^{(j)} - \avg{x}}^2
\end{equation*}

\begin{definition}
  Внутригрупповой (остаточной) дисперсией называется среднее групповых
  дисперсий.

  \begin{equation*}
    \varianceB = \frac{1}{n} \sum_{j = 1}^k n_j \varianceD^{(j)}
  \end{equation*}
\end{definition}

\begin{definition}
  Межгрупповой (факторной) дисперсией (или дисперсией выборочных средних)
  называется величина

  \begin{equation*}
    \varianceM
    = \frac{1}{n} \sum_{j = 1}^k \prh{\avg{x}^{(j)} - \avg{x}}^2 n_j
  \end{equation*}
\end{definition}

\begin{theorem}[О разложении дисперсии] \label{thr:variance-expansion}
  \begin{equation*}
    \varianceO = \varianceM + \varianceB
  \end{equation*}
\end{theorem}

\begin{proof}
  Это можно доказать чисто алгебраически исходя из определений.
\end{proof}

\begin{remark}
  Ясно, что чем большее влияние оказывает \(Z\) на результат \(X\), тем больше
  получаются отдельные выборочные средние, а значит растет доля межгрупповой
  дисперсии в данной сумме. Таким образом величина корреляции характеризуется
  отношением \(\display{\frac{\varianceM}{\varianceO}}\). Если зависимость
  функциональная, то эта дробь будет равна единице.
\end{remark}

\subheader{Проверка гипотезы о влиянии фактора}

Предполагаем, что случайная величина \(X\) имеет нормальное распределение и
фактор \(Z\) может влиять только на среднее значение, но не на дисперсию и тип
распределения. Из-за этого можно считать, что данные независимые \(k\)-выборки
при различных уровнях фактора \(Z\) также имеют нормальное распределение с
одинаковой дисперсией, т.е. \(X^{(j)} \in \ndist{a_j}{\sigma^2}\).

Проверяется основная гипотеза \(H_0\) о том, что \(a_1 = \dotsc = a_j\), т.е.
\(Z\) не влияет на среднее \(X\), против альтернативной \(H_1\) о том, что
фактор \(Z\) влияет на среднее \(X\). По \ref{lem:base-theorem-3} для каждой из
\(k\) выборок имеем

\begin{equation*}
  \sum_{i = 1}^n \prh{\frac{x - \avg{x}}{\sigma}}^2
  = \frac{n \svarianceD}{\sigma^2}
  \in \Hdist{n_j - 1}
  \qquad 1 \le j \le k
\end{equation*}

Т.к. распределение \quote{хи-квадрат} устойчиво относительно суммирования, то

\begin{equation*}
  \sum_{j = 1}^k \frac{n_j \varianceD^{(j)}}{\sigma^2}
  = \frac{n \varianceD_\text{в}}{\sigma^2}
  \in \Hdist{n - k}
  \text{ т.к. }
  \under{(n_1 - 1) + \dotsc + (n_k - 1)}{k \text{ раз}} = n - k
\end{equation*}

Пусть основная гипотеза \(H_0\) верна, тогда все данные можно считать одной
выборкой объема \(n\) и опять по \ref{lem:base-theorem-3} имеем

\begin{equation*}
  \frac{n \varianceD_\text{О}}{\sigma^2} \in \Hdist{n - 1}
\end{equation*}

Согласно \ref{thr:variance-expansion} получаем

\begin{equation*}
  \begin{aligned}
    \varianceO = \varianceM + \varianceB
  \\
    \frac{n \varianceO}{\sigma^2} = \frac{n \varianceM}{\sigma^2}
      + \frac{n \varianceB}{\sigma^2}
  \end{aligned}
\end{equation*}

Т.к. левая часть имеет распределение \(\Hdist{n - 1}\), а второе слагаемое в
правой части имеет распределение \(\Hdist{n - k}\), то первое слагаемое в правой
части имеет распределение \(\Hdist{k - 1}\). Итак, при первой основной гипотезе
\(H_0\) получили, что

\begin{equation*}
  \frac{n \varianceM}{\sigma^2} \in \Hdist{k - 1}
  \qquad
  \frac{n \varianceB}{\sigma^2} \in \Hdist{n - k}
\end{equation*}

Значит

\begin{equation*}
  \frac{n \varianceM}{\sigma^2 (k - 1)}
  \cdot \frac{\sigma^2 (n - k)}{n \varianceB}
  = \frac{n - k}{k - 1} \cdot \frac{\varianceM}{\varianceB}
    \in \Fdist{k - 1}{n - k}
\end{equation*}

Таким образом в качестве статистики критерия берем функцию

\begin{equation*}
  K = \frac{n - k}{k - 1} \cdot \frac{\varianceM}{\varianceB}
\end{equation*}

В результате получили критерий: пусть \(\tcrit\) это квантиль распределения
\(\Fdist{k - 1}{n - k}\) уровня значимости \(\alpha\), тогда

\begin{equation*}
  \begin{cases}
    H_0, & K < \tcrit \\
    H_1, & K \ge \tcrit
  \end{cases}
\end{equation*}
