\subsection{%
  Лекция \texttt{24.??.??}.%
}

\subheader{Построение и анализ уравнения множественной линейной регрессии}

Пусть выявлена зависимость результата \(X\) от факторов \(Z_1, \dotsc, Z_m\).
Проведено \(n \ge m\) экспериментов и получены экспериментальные данные
результата \(\sample{X} = \prh{X_1, \dotsc, X_n}\) при соответствующих значениях
факторов \(\sample{Z}^{(k)} = \prh{Z_1^{(k)}, \dotsc, Z_n^{(k)}}\).
Предполагаем, что зависимость \(X\) от всех факторов линейная. Требуется по этим
данным построить модель наилучшим образом объясняющую и предсказывающую
поведение \(X\).

\subheader{Мультиколлинеарность}

\begin{definition}
  Мультиколлинеарность это наличие заметной корреляции между всеми или
  некоторыми факторами.
\end{definition}

Неприятные последствия мультиколлинеарности:

\begin{enumerate}
\item
  Оценки параметров становятся ненадежными, имеют большие стандартные ошибки и
  малую значимость, причем даже в том случае, когда модель в целом имеет высокую
  значимость.

\item
  Небольшое изменение исходных данных может привести к существенному изменению
  оценок регрессии.

\item
  Трудно выявить изолированное влияние конкретного фактора на результат и
  физический/экономический/т.д. смысл этого влияния.
\end{enumerate}

\subheader{Начальный отбор факторов в уравнение регрессии}

Построим корреляционную матрицу, состоящую из коэффициентов линейной корреляции.

\begin{equation*}
  P = \mtxp{
    1             & \rho_{x, z_1}   & \rho_{x, z_2}   & \dotsc & \rho_{x, z_m}
  \\
    \rho_{z_1, x} & 1               & \rho_{z_1, z_2} & \dotsc & \rho_{z_1, z_m}
  \\
    \rho_{z_2, x} & \rho_{z_2, z_1} & 1               & \dotsc & \rho_{z_2, z_m}
  \\
    \vdots        &  \vdots         & \vdots          & \ddots & \vdots
  \\
    \rho_{z_m, x} & \rho_{z_m, z_1} & \rho_{z_m, z_2} & \dotsc & 1
  }
\end{equation*}

Выбираем фактор, наиболее коррелирующий с \(X\). Далее добавляем в модель
факторы, которые с одной стороны имеют б\'oльшую корреляцию с результатом \(X\),
а с другой стороны наименее коррелированны с факторами, которые уже включены в
модель.

\begin{example}
  Пусть дана корреляционная таблица.
  
  \begin{equation*}
    \mtx{
          & X     & Z_1   & Z_2   & Z_3   \\
      X   & 1     & 0.81  & 0.85  & -0.65 \\
      Z_1 & 0.81  & 1     & 0.93  & -0.38 \\
      Z_2 & 0.85  & 0.93  & 1     & -0.28 \\
      Z_3 & -0.65 & -0.38 & -0.28 & 1
    }
  \end{equation*}

  Согласно алгоритму естественно включить фактор \(Z_2\), т.к. он имеет
  наибольшую корреляцию с \(X\). Далее логично включить фактор \(Z_3\), т.к.
  \(Z_1\) сильно коррелирован с уже включенным фактором \(Z_2\). Итого выбираем
  факторы \(Z_2\) и \(Z_3\).
\end{example}

\subheader{Анализ уравнения линейной регрессии}

Пусть после отсева осталось \(k\) факторов. Теоретическая модель регрессии имеет
вид

\begin{equation*}
  X = \beta_0 + \beta_1 z_1 + \dotsc + \beta_k z_k + \epsilon
\end{equation*}

где \(\epsilon\)~--- случайный член, отражающий влияние неучтенных факторов,
возможную нелинейность модели, случай и т.д.

Методом наименьших квадратов построили модель

\begin{equation*}
  \widehat{X} = b_0 + b_1 z_1 + \dotsc + b_k z_k + \widehat{\epsilon}
\end{equation*}

Предполагаем, что \(\forall i \given \epsilon_i \in \ndist{0}{\sigma^2}\) и
независимы. Согласно пункту 3 в \ref{thr:md-err} получили несмещенную оценку для
\(\sigma^2\) в виде

\begin{equation*}
  S^2 = \frac{1}{n - k - 1} \sum_{i = 1}^{n} \widehat{\epsilon}_i^2
\end{equation*}

\begin{definition}
  \(S\) это стандартная ошибка регрессии.
\end{definition}

Из \ref{rem:variance-b-i} получили \(\display{\variance{b_i} = \sigma^2
\prh{A^{-1}}_{i, i}}\), где \(A = Z Z^T\), а \(Z\) это матрица плана. Тогда

\begin{equation*}
  S_{b_i} = S \sqrt{\prh{A^{-1}}_{i, i}}
\end{equation*}

Это стандартные ошибки коэффициентов регрессии \(b_i\).

\subheader{Уравнение регрессии в стандартных масштабах}

\begin{remark}
  При обычном уравнении регрессии по коэффициентам \(b_i\) нельзя судить о силе
  влияния фактора на результат \(X\), т.к. факторы могут быть разной природы и
  иметь различные единицы измерения.
\end{remark}

Стандартизация данных. Пусть имеется выборка \(\prh{X_1, \dotsc, X_n}\)
случайной величины \(X\). Заменяем ее выборкой \(t_x \colon X_i \to \frac{X_i -
\avg{x}}{\widehat{\sigma}_x}\), которую можно считать выборкой случайной
величины \(t_x = \frac{X - \expected{X}}{\sigma_X}\), не имеющей единиц
измерения.

\begin{remark}
  Очевидно, что \(\avg{t_x} = 0\) и \(\widehat{\varianceD}_{t_x} = 1\).
\end{remark}

\begin{lemma}
  \begin{equation*}
    \widehat{\rho}_{X, Y} = \widehat{\rho}_{t_x, t_y} = \avg{t_x t_y}
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{equation*}
    \widehat{\rho}_{X, Y}
    = \frac{\scov{X}{Y}}{\widehat{\sigma}_x \widehat{\sigma}_y}
    = \frac{\frac{1}{n}
        - \sum_{i = 1}^n \prh{x_i - \avg{x} \prh{y_i - \avg{y}}}}
      {\widehat{\sigma}_x \widehat{\sigma}_y}
    = \frac{1}{n} \sum_{i = 1}^n 
      \frac{x_i - \avg{x}}{\widehat{\sigma}_x}
      \frac{y_i - \avg{y}}{\widehat{\sigma}_y}
    = \frac{1}{n} \sum_{i = 1}^n t_x t_y 
    = \avg{t_x t_y}
  \end{equation*}

  Равенство \(\widehat{\rho}_{t_x, t_y} = \avg{t_x t_y}\) будет выполняться,
  т.к. \(\avg{t_x} = \avg{t_y} = 0\) и \(\widehat{\varianceD}_{t_x} =
  \widehat{\varianceD}_{t_y} = 1\) в силу стандартизованности этих величин.
\end{proof}

Все значения исследуемых признаков и результата стандартизируем

\begin{equation*}
  \begin{aligned}
    t_j = \frac{z_j^{(i)} - \avg{z}_j}{\widehat{\sigma}_{z_j}}
    & \qquad
    1 \le i \le k
  \\
    t_x = \frac{x_i - \avg{x}}{\widehat{\sigma}_x} &
  \end{aligned}
\end{equation*}

При линейной модели регрессии можно все величины в уравнении регрессии заменить
на стандартизованные, т.к. все операции при стандартизации линейные. В
результате получим так называемое уравнение в стандартных масштабах.

\begin{equation*}
  t_x = \gamma_1 t_1 + \dotsc + \gamma_k t_k
\end{equation*}

\begin{remark}
  Заметим, что свободного члена нет, т.к. \(\avg{t}_x = \avg{t}_1 = \dotsc =
  \avg{t}_k = 0\).
\end{remark}

\begin{lemma}
  При стандартизации система нормальных уравнений приобретает более простой вид.

  \begin{equation*}
    \begin{cases}
      \gamma_1 + \rho_{z_1, z_2} \gamma_2 + \dotsc + \rho_{z_1, z_k} \gamma_k
        = \rho_{z_1, x}
    \\
      \rho_{z_2, z_1} \gamma_1 + \gamma_2 + \dotsc + \rho_{z_2, z_k} \gamma_k
        = \rho_{z_2, x}
    \\
      \dotsc
    \\
      \rho_{z_k, z_1} \gamma_1 + \rho_{z_k, z_2} \gamma_2 + \dotsc + \gamma_k
        = \rho_{z_k, x}
    \end{cases}
  \end{equation*}

  В матричной форме это можно записать как \(P \Gamma = P_x\), где \(P\) это
  матрица корреляций между факторами, \(\Gamma = \prh{\gamma_1, \dotsc,
  \gamma_k}^T\) и \(P_x = \prh{\rho_{x, z_1}, \dotsc, \rho_{x, z_k}}^T\).
\end{lemma}

\begin{proof}
  Действительно, нормальное уравнение регрессии имело вид \(A B = Z \vec{X}\),
  где \(A = Z Z^T\), а \(Z\) это матрица плана. Допустим, что все данные
  стандартизированны, тогда \(i\)-тый элемент столбца \(Z \vec{X}\) имеет вид

  \begin{equation*}
    \prh{Z_1^{(i)}, \dotsc, Z_n^{(i)}} \cdot \mtxp{
      X_1 \\ \vdots \\ X_n
    }
    = n \avg{Z^{(i)} X}
    = n \rho_{z_i, x}
  \end{equation*}

  При этом элемент \(a_{i, j}\) матрицы \(A\) будет равен

  \begin{equation*}
    a_{i, j}
    = \prh{Z_1^{(i)}, \dotsc, Z_n^{(i)}} \cdot \mtxp{
      Z_1^{(j)} \\ \vdots \\ Z_n^{(j)}
    }
    = \begin{cases}
      n \avg{z_i z_j} = n \rho_{z_i, z_j}  & i \neq j \\
      n \avg{z_i^2} = n \variance{z_i} = n & i = j
    \end{cases}
  \end{equation*}

  После сокращения \(n\) в левой и правой части получаем искомую систему
  уравнений.
\end{proof}

Переход от уравнения в стандартных масштабах к обычному уравнению регрессии и
обратно можно выполнить по следующим формулам.

\begin{equation*}
  b_i = \gamma_i \frac{\sigma_x}{\sigma_{z_i}}
  \qquad
  b_0 = \avg{X} - \sum_{i = 1}^n b_i \avg{z_i} 
\end{equation*}

\begin{remark}
  В случае парной линейной регрессии уравнение в стандартных масштабах имеет вид
  \(t_x = \rho t_z\).
\end{remark}

\subheader{Смысл стандартизованных коэффициентов}

Коэффициент \(\gamma_i\) на какую часть своего среднеквадратического отклонения
\(\sigma_x\) изменится результат \(X\) при изменении фактора \(Z_i\) на величину
своего среднеквадратического отклонения \(\sigma_{z_i}\) (при неизменных
значениях других факторов). При мультиколлинеарности факторы оказывают не только
прямое влияние на результат, но и косвенное (через влияние на другие факторы).
Стандартизованный коэффициент \(\gamma_i\) можно трактовать как показатель
прямого влияния, а остальные слагаемые в уравнении как результат косвенного
влияния.

\begin{equation*}
  \rho_{z_i, z_1} \gamma_1
    + \rho_{z_i, z_2} \gamma_2
    + \dotsc
    + \rho_{z_i, z_{i - 1}} \gamma_{i - 1}
    + \gamma_i
    + \rho_{z_i, z_{i + 1}} \gamma_{i + 1}
    + \dotsc
    + \rho_{z_i, z_k} \gamma_k
  = \rho_{z_i, x}
\end{equation*}

Результат \(\rho_{z_i, x}\) показывает величину полного влияния, а \(\gamma_i\)
можно грубо трактовать как величину прямого влияния. Все остальное можно
трактовать как косвенное влияние.

\begin{remark}
  Для измерения тесноты линейной связи между фактором и результатом при
  устранении влияния остальных факторов есть понятия коэффициентов частной
  корреляции. Например, при \(k = 2\)

  \begin{equation*}
    \rho_{x, z_1 \mid z_2}
    = \frac{\rho_{x, z_1} - \rho_{x, z_2} \rho_{z_1, z_2}}
      {\sqrt{\prh{1 - \rho_{z_1, z_2}^2} \prh{1 - \rho_{x, z_2}^2}}}
    \qquad
    \rho_{x, z_2 \mid z_1}
    = \frac{\rho_{x, z_2} - \rho_{x, z_1} \rho_{z_1, z_2}}
      {\sqrt{\prh{1 - \rho_{z_1, z_2}^2} \prh{1 - \rho_{x, z_1}^2}}}
  \end{equation*}
\end{remark}

\subheader{Коэффициенты детерминации и множественной корреляции}

Допустим, что как и в случае парной линейной регрессии дисперсию результата
\(X\) можно разложить на объясненную и необъясненную составляющую, т.е.

\begin{equation*}
  \variance{X} = \variance{\widehat{X}} + \variance{\widehat{\epsilon}}
\end{equation*}

где \(\variance{\widehat{X}}\) это дисперсия расчетных значений по построенной
модели МНК, а \(\variance{\widehat{\epsilon}}\) это дисперсия экспериментальных
ошибок.

\begin{definition}
  Коэффициентом детерминации \(R^2\) называется величина

  \begin{equation*}
    R^2 = 1 - \frac{\variance{\widehat{\epsilon}}}{\variance{X}}
  \end{equation*}
\end{definition}

\begin{remark}
  Ясно, что \(0 \le R^2 \le 1\), причем чем больше \(R^2\), тем лучше качество
  модели. Если \(R^2 = 1\), то \(\variance{\widehat{\epsilon}} = 0\), т.е.
  \(\widehat{\epsilon} \equiv 0\), значит все экспериментальные данные легли на
  гиперплоскость регрессии. Если \(R^2 = 0\), то \(\variance{\widehat{X}} = 0\),
  т.е. \(\widehat{X} = \avg{X}\), значит \(b_0 = \avg{X}, b_1 = \dotsc = b_k =
  0\). Такая модель ничего не объясняет.
\end{remark}

\begin{remark}
  В случае линейного уравнения регрессии \(R^2 = \sum_{i = 1}^k \gamma_i
  \rho_{x, z_i}\), где \(\gamma_i\) это стандартизованные коэффициенты.
\end{remark}

\begin{definition}
  \(R\) называется коэффициентом множественной корреляции.
\end{definition}

\begin{remark}
  При добавлении в модель новых факторов \(R^2\) всегда вырастет, однако не
  всегда эти факторы следует вводить в модель, поэтому для выяснения того,
  следует ли это делать, существует скорректированный коэффициент детерминации
  \(\bar{R^2}\).

  \begin{equation*}
    \bar{R^2} = 1 - \frac{n - 1}{n - k - 1}
      \cdot \frac{\variance{\widehat{\epsilon}}}{\variance{X}}
  \end{equation*}

  где \(n\) это число экспериментов, а \(k\) это число факторов в модели.
\end{remark}

\subheader{\(F\)-тест: проверка гипотезы о значимости уравнения регрессии в
целом}

Проверяется основная гипотеза \(H_0\) о том, что \(R_{\text{т}}^2 = 0\) (т.е.
уравнение в целом не значимо), против альтернативной \(H_1\) о том, что
\(R_{\text{т}}^2 \neq 0\).

\begin{theorem}
  Если нулевая гипотеза верна, то

  \begin{equation*}
    F = \frac{R^2}{1 - R^2} \cdot \frac{n - k - 1}{k}
    \in \Fdist{k}{n - k - 1}
  \end{equation*}
\end{theorem}

Пусть \(\tcrit\) это квантиль распределения \(\Fdist{k}{n - k - 1}\) уровня
значимости \(\alpha\), тогда

\begin{equation*}
  \begin{cases}
    H_0, & F < \tcrit \\
    H_1, & F \ge \tcrit
  \end{cases}
\end{equation*}

\subheader{\(T\)-тест: проверка гипотезы о значимости отдельного коэффициента
регрессии}

Проверяется основная гипотеза \(H_0\) о том, что \(\beta_i = 0\), против
альтернативной \(H_1\) о том, что \(\beta_i \neq 0\).

\begin{theorem}
  Если нулевая гипотеза верна, то

  \begin{equation*}
    T_i = \frac{b_i}{S_{b_i}}
    \in \Tdist{n - k - 1}
  \end{equation*}
\end{theorem}

Пусть \(\tcrit\) это квантиль распределения \(\abs{\Tdist{n - k - 1}}\) уровня
значимости \(\alpha\), тогда

\begin{equation*}
  \begin{cases}
    H_0, & \abs{T_i} < \tcrit \\
    H_1, & \abs{T_i} \ge \tcrit
  \end{cases}
\end{equation*}

\begin{remark}
  Данный критерий применяется для отсева несущественных факторов.  
\end{remark}

\begin{remark}
  При мультиколлинеарности может оказаться так, что все коэффициенты по
  отдельности статистически не значимы, в то время как модель в целом имеет
  высокую значимость.
\end{remark}
